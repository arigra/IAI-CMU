{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faster RCNN implementation for radar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import time\n",
    "import json\n",
    "import torch\n",
    "import pickle\n",
    "import random\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from torchvision import models, transforms\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import functional as F\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import config info "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(config_path):\n",
    "    with open(config_path, 'r') as config_file:\n",
    "        content = config_file.read()\n",
    "        if not content.strip():\n",
    "            raise ValueError(\"Configuration file is empty\")\n",
    "        config = json.loads(content)\n",
    "    return config\n",
    "\n",
    "\n",
    "config_path = \"/home/hawk/Desktop/objectDetection/Object_detection/configs/config.json\"\n",
    "config = load_config(config_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load RADDet data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ---- RAD ----\n",
      "number of RAD files loaded: 126\n",
      "each RAD file is of size: (256, 256, 64) which stands for: (Range, Azimuth, Doppler)\n",
      "\n",
      "\n",
      " ---- GT ----\n",
      "number of GT files loaded: 126\n",
      "each GT file is a dict with length 3: classes, boxes and cart_boxes. an example:\n",
      "{'classes': ['truck', 'car', 'car', 'car'], 'boxes': array([[177.5, 173. ,  30. ,  28. ,  51. ,  25. ],\n",
      "       [202. , 217.5,  32. ,   9. ,  34. ,  17. ],\n",
      "       [ 66. ,  95. ,  40.5,   5. ,   7. ,   4. ],\n",
      "       [ 15.5,  89.5,  38. ,   4. ,   8. ,   5. ]]), 'cart_boxes': array([[183.5, 283.5,  30. ,  30. ],\n",
      "       [218. , 290.5,  19. ,  10. ],\n",
      "       [ 72.5, 206. ,   6. ,  11. ],\n",
      "       [ 26.5, 183. ,   8. ,  13. ]])}\n",
      "\n",
      "\n",
      " ---- Stereo ----\n",
      "Number of loaded images: 126\n"
     ]
    }
   ],
   "source": [
    "# Define folder paths\n",
    "RAD_folder_path = config[\"RAD_folder_path\"]\n",
    "stereo_folder_path = config[\"stereo_folder_path\"]\n",
    "GT_folder_path = config[\"GT_folder_path\"]\n",
    "\n",
    "# Numpy files\n",
    "all_RAD_files = os.listdir(RAD_folder_path)\n",
    "RAD_files = [file for file in all_RAD_files if file.endswith('.npy')]\n",
    "RAD_files_paths = [os.path.join(RAD_folder_path, file) for file in RAD_files]\n",
    "\n",
    "# Ground truth files\n",
    "all_GT_files = os.listdir(GT_folder_path)\n",
    "GT_files = [file for file in all_GT_files if file.endswith('.pickle')]\n",
    "GT_files_paths = [os.path.join(GT_folder_path, file) for file in all_GT_files]\n",
    "\n",
    "# Stereo image files\n",
    "all_stereo_files = os.listdir(stereo_folder_path)\n",
    "stereo_files = [file for file in all_stereo_files if file.endswith('.jpg')]\n",
    "stereo_files_paths = [os.path.join(stereo_folder_path, file) for file in all_stereo_files]\n",
    "\n",
    "all_loaded_RAD_files = []\n",
    "for file in RAD_files_paths:\n",
    "    loaded_file = np.load(file)\n",
    "    all_loaded_RAD_files.append(loaded_file)\n",
    "print(' ---- RAD ----')\n",
    "print(f'number of RAD files loaded: {len(all_loaded_RAD_files)}')\n",
    "print(f'each RAD file is of size: {all_loaded_RAD_files[0].shape} which stands for: (Range, Azimuth, Doppler)')\n",
    "print('\\n')\n",
    "\n",
    "all_loaded_GT_files = []\n",
    "for file in GT_files_paths:\n",
    "    with open(file, 'rb') as f:  \n",
    "        ground_truth = pickle.load(f) \n",
    "    all_loaded_GT_files.append(ground_truth)\n",
    "print(' ---- GT ----')\n",
    "print(f'number of GT files loaded: {len(all_loaded_GT_files)}')\n",
    "print(f'each GT file is a dict with length 3: classes, boxes and cart_boxes. an example:')\n",
    "print(all_loaded_GT_files[0])\n",
    "print('\\n')\n",
    "\n",
    "all_loaded_stereo_files = []\n",
    "for jpg_file in stereo_files_paths:\n",
    "    img = Image.open(jpg_file)\n",
    "    img = img.convert('RGB')\n",
    "    all_loaded_stereo_files.append(img)\n",
    "print(' ---- Stereo ----')\n",
    "print(f\"Number of loaded images: {len(all_loaded_stereo_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a custom RADDet dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readAndEncodeGtRD(gt_instance, rd_shape):\n",
    "    x_shape, y_shape = rd_shape[1], rd_shape[0]\n",
    "    boxes = gt_instance[\"boxes\"]\n",
    "    classes = gt_instance[\"classes\"]\n",
    "    new_boxes = []\n",
    "    new_classes = []\n",
    "    for (box, class_) in zip(boxes, classes):\n",
    "        yc, xc, h, w = box[0], box[2], box[3], box[5]\n",
    "        y1, y2, x1, x2 = int(yc - h / 2), int(yc + h / 2), int(xc - w / 2), int(xc + w / 2)\n",
    "        if x1 < 0:\n",
    "            x1 += x_shape\n",
    "            box1 = [y1 / y_shape, x1 / x_shape, y2 / y_shape, x_shape / x_shape]\n",
    "            box2 = [y1 / y_shape, 0 / x_shape, y2 / y_shape, x2 / x_shape]\n",
    "            new_boxes.append(box1)\n",
    "            new_classes.append(class_)\n",
    "            new_boxes.append(box2)\n",
    "            new_classes.append(class_)\n",
    "        elif x2 >= x_shape:\n",
    "            x2 -= x_shape\n",
    "            box1 = [y1 / y_shape, x1 / x_shape, y2 / y_shape, x_shape / x_shape]\n",
    "            box2 = [y1 / y_shape, 0 / x_shape, y2 / y_shape, x2 / x_shape]\n",
    "            new_boxes.append(box1)\n",
    "            new_classes.append(class_)\n",
    "            new_boxes.append(box2)\n",
    "            new_classes.append(class_)\n",
    "        else:\n",
    "            new_boxes.append([y1 / y_shape, x1 / x_shape, y2 / y_shape, x2 / x_shape])\n",
    "            new_classes.append(class_)\n",
    "    return new_boxes, new_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complexTo2channels(target_array):\n",
    "    assert target_array.dtype == np.complex64\n",
    "    output_array = getMagnitude(target_array)\n",
    "    output_array = getLog(output_array)\n",
    "    return output_array\n",
    "\n",
    "def getMagnitude(target_array, power_order=2):\n",
    "    target_array = np.abs(target_array)\n",
    "    target_array = pow(target_array, power_order)\n",
    "    return target_array\n",
    "\n",
    "def getLog(target_array, scalar=1., log_10=True):\n",
    "    if log_10:\n",
    "        return scalar * np.log10(target_array + 1.)\n",
    "    else:\n",
    "        return target_array\n",
    "\n",
    "def getSumDim(target_array, target_axis):\n",
    "    output = np.sum(target_array, axis=target_axis)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RaddetDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for RADDet dataset.\"\"\"\n",
    "    \n",
    "    def __init__(self, RAD_files, GT_files, stereo_files, transform=None):\n",
    "        self.transform = transform\n",
    "        self.classes_list = [\"person\", \"bicycle\", \"car\", \"motorcycle\", \"bus\", \"truck\"]\n",
    "        self.RAD_maps = RAD_files\n",
    "        self.GT_data = GT_files\n",
    "        self.stereo_data = stereo_files\n",
    "        self.global_mean_log = 3.2438383\n",
    "        self.global_variance_log = 6.8367246\n",
    "        self.global_max_log = 10.0805629\n",
    "        self.global_min_log = 0.0\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.RAD_maps)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        RAD_complex = self.RAD_maps[idx]\n",
    "        \n",
    "        RAD_data = complexTo2channels(RAD_complex)\n",
    "        RAD_data = (RAD_data - self.global_mean_log) / self.global_variance_log\n",
    "        \n",
    "        gt_instances = self.GT_data[idx]\n",
    "\n",
    "        RD_data = getSumDim(RAD_data, target_axis=1)\n",
    "\n",
    "        bboxes, classes = readAndEncodeGtRD(gt_instances, RD_data.shape)\n",
    "        #seq_id = int(RAD_filename.split('/')[-2].split('_')[-1])\n",
    "        \n",
    "        objects = []\n",
    "        for box, class_ in zip(bboxes, classes):\n",
    "            ymin, xmin, ymax, xmax = box\n",
    "            area = (xmax - xmin) * (ymax - ymin)\n",
    "            objects.append({\n",
    "                'bbox': [ymin, xmin, ymax, xmax],\n",
    "                'label': self.classes_list.index(class_),\n",
    "                'area': area,\n",
    "            })\n",
    "        \n",
    "        image_filename = self.stereo_data[idx]\n",
    "        image = plt.imread(image_filename)\n",
    "\n",
    "\n",
    "        sample = {\n",
    "            'spectrum': torch.tensor(RD_data, dtype=torch.float32),\n",
    "            'image': image,\n",
    "            'image_filename': image_filename,\n",
    "            'objects': objects,\n",
    "        }\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RADDet Dataset Sample Details:\n",
      "\n",
      "\n",
      "Sample spectrum shape: torch.Size([256, 64])\n",
      "Image filename: /home/hawk/Desktop/data/Raddet/train/stereo_image/part_9/004115.jpg\n",
      "Number of objects: 3\n",
      "Object bbox: [0.27734375, 0.859375, 0.35546875, 0.90625], label: 2, area: 0.003662109375\n",
      "Object bbox: [0.33203125, 0.0, 0.36328125, 0.03125], label: 2, area: 0.0009765625\n",
      "Object bbox: [0.0703125, 0.0, 0.08203125, 0.109375], label: 2, area: 0.00128173828125\n"
     ]
    }
   ],
   "source": [
    "train_dataset = RaddetDataset(all_loaded_RAD_files, all_loaded_GT_files, stereo_files_paths)#all_loaded_stereo_files)\n",
    "sample = train_dataset[34]\n",
    "\n",
    "print(\"RADDet Dataset Sample Details:\")\n",
    "print(\"\\n\")\n",
    "print(f'Sample spectrum shape: {sample[\"spectrum\"].shape}')\n",
    "print(f'Image filename: {sample[\"image_filename\"]}')\n",
    "print(f'Number of objects: {len(sample[\"objects\"])}')\n",
    "for obj in sample[\"objects\"]:\n",
    "    print(f'Object bbox: {obj[\"bbox\"]}, label: {obj[\"label\"]}, area: {obj[\"area\"]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Iteration 1, Loss: 4.70062780380249\n",
      "Epoch 1, Iteration 2, Loss: 2.171436309814453\n",
      "Epoch 1, Iteration 3, Loss: 1.4171062707901\n",
      "Epoch 1, Iteration 4, Loss: 1.3254084587097168\n",
      "Epoch 1, Iteration 5, Loss: 0.9500620365142822\n",
      "Epoch 1, Iteration 6, Loss: 0.6933348178863525\n",
      "Epoch 1, Iteration 7, Loss: 0.9877203702926636\n",
      "Epoch 1, Iteration 8, Loss: 0.537901759147644\n",
      "Epoch 1, Iteration 9, Loss: 0.404548704624176\n",
      "Epoch 1, Iteration 10, Loss: 0.5297384858131409\n",
      "Epoch 1, Iteration 11, Loss: 0.5876920223236084\n",
      "Epoch 1, Iteration 12, Loss: 0.6600764393806458\n",
      "Epoch 1, Iteration 13, Loss: 0.567037045955658\n",
      "Epoch 1, Iteration 14, Loss: 0.33590707182884216\n",
      "Epoch 1, Iteration 15, Loss: 0.8233029842376709\n",
      "Epoch 1, Iteration 16, Loss: 0.6091045141220093\n",
      "Epoch 1, Iteration 17, Loss: 0.4139969050884247\n",
      "Epoch 1, Iteration 18, Loss: 0.6452269554138184\n",
      "Epoch 1, Iteration 19, Loss: 0.9290531277656555\n",
      "Epoch 1, Iteration 20, Loss: 0.5474666357040405\n",
      "Epoch 1, Iteration 21, Loss: 0.5110000967979431\n",
      "Epoch 1, Iteration 22, Loss: 0.34531575441360474\n",
      "Epoch 1, Iteration 23, Loss: 0.4465793967247009\n",
      "Epoch 1, Iteration 24, Loss: 0.2272900640964508\n",
      "Epoch 1, Iteration 25, Loss: 0.2812845706939697\n",
      "Epoch 1, Iteration 26, Loss: 0.295185923576355\n",
      "Epoch 1, Iteration 27, Loss: 0.19693806767463684\n",
      "Epoch 1, Iteration 28, Loss: 0.22373510897159576\n",
      "Epoch 1, Iteration 29, Loss: 0.22028085589408875\n",
      "Epoch 1, Iteration 30, Loss: 0.29365280270576477\n",
      "Epoch 1, Iteration 31, Loss: 0.429818719625473\n",
      "Epoch 1, Iteration 32, Loss: 0.21327409148216248\n",
      "Epoch 1, Iteration 33, Loss: 0.1725313663482666\n",
      "Epoch 1, Iteration 34, Loss: 0.2691100239753723\n",
      "Epoch 1, Iteration 35, Loss: 0.424748033285141\n",
      "Epoch 1, Iteration 36, Loss: 0.2163306325674057\n",
      "Epoch 1, Iteration 37, Loss: 0.32568684220314026\n",
      "Epoch 1, Iteration 38, Loss: 0.43859535455703735\n",
      "Epoch 1, Iteration 39, Loss: 0.38755956292152405\n",
      "Epoch 1, Iteration 40, Loss: 0.33521875739097595\n",
      "Epoch 1, Iteration 41, Loss: 0.2590823173522949\n",
      "Epoch 1, Iteration 42, Loss: 0.25797539949417114\n",
      "Epoch 1, Iteration 43, Loss: 0.2803560197353363\n",
      "Epoch 1, Iteration 44, Loss: 0.5912248492240906\n",
      "Epoch 1, Iteration 45, Loss: 0.43314164876937866\n",
      "Epoch 1, Iteration 46, Loss: 0.1265878826379776\n",
      "Epoch 1, Iteration 47, Loss: 0.4570983052253723\n",
      "Epoch 1, Iteration 48, Loss: 0.3652297854423523\n",
      "Epoch 1, Iteration 49, Loss: 0.27734696865081787\n",
      "Epoch 1, Iteration 50, Loss: 0.19336053729057312\n",
      "Epoch 1, Iteration 51, Loss: 0.19583061337471008\n",
      "Epoch 1, Iteration 52, Loss: 0.688130259513855\n",
      "Epoch 1, Iteration 53, Loss: 0.3894643783569336\n",
      "Epoch 1, Iteration 54, Loss: 0.20648504793643951\n",
      "Epoch 1, Iteration 55, Loss: 0.30968180298805237\n",
      "Epoch 1, Iteration 56, Loss: 0.17225201427936554\n",
      "Epoch 1, Iteration 57, Loss: 0.12609459459781647\n"
     ]
    }
   ],
   "source": [
    "def get_transform():\n",
    "    def transform(sample):\n",
    "        spectrum = sample['spectrum']\n",
    "        image = sample['image']\n",
    "        objects = sample['objects']\n",
    "        \n",
    "        image = F.to_tensor(image)\n",
    "        return {'spectrum': spectrum, 'image': image, 'objects': objects}\n",
    "    \n",
    "    return transform\n",
    "\n",
    "# Function to validate bounding boxes\n",
    "def is_valid_bbox(bbox):\n",
    "    ymin, xmin, ymax, xmax = bbox\n",
    "    return (xmax - xmin) > 0 and (ymax - ymin) > 0\n",
    "\n",
    "# Load the dataset with transformations\n",
    "train_dataset = RaddetDataset(all_loaded_RAD_files, all_loaded_GT_files, stereo_files_paths, transform=get_transform())\n",
    "\n",
    "# Define the data loader\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=lambda x: x)\n",
    "\n",
    "# Define the model\n",
    "def get_model(num_classes):\n",
    "    # Load a pre-trained model\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    \n",
    "    # Get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    \n",
    "    # Replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Number of classes (including background)\n",
    "num_classes = len(train_dataset.classes_list) + 1\n",
    "\n",
    "# Get the model\n",
    "model = get_model(num_classes)\n",
    "\n",
    "# Move model to the right device\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# Learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    i = 0\n",
    "    for samples in train_loader:\n",
    "        images = list(sample['image'].to(device) for sample in samples)\n",
    "        targets = []\n",
    "        for sample in samples:\n",
    "            d = {}\n",
    "            valid_objects = [obj for obj in sample['objects'] if is_valid_bbox(obj['bbox'])]\n",
    "            if len(valid_objects) == 0:\n",
    "                continue\n",
    "            d['boxes'] = torch.tensor([obj['bbox'] for obj in valid_objects], dtype=torch.float32).to(device)\n",
    "            d['labels'] = torch.tensor([obj['label'] for obj in valid_objects], dtype=torch.int64).to(device)\n",
    "            targets.append(d)\n",
    "        \n",
    "        if len(targets) == 0:\n",
    "            continue\n",
    "\n",
    "        # Forward pass\n",
    "        loss_dict = model(images, targets)\n",
    "\n",
    "        # Calculate the total loss\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        print(f'Epoch {epoch+1}, Iteration {i+1}, Loss: {losses.item()}')\n",
    "        i += 1\n",
    "    \n",
    "    # Update the learning rate\n",
    "    lr_scheduler.step()\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faster RCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(batch):\n",
    "    inputs = [item['spectrum'].unsqueeze(1) for item in batch]  # Add channel dimension\n",
    "    labels = [torch.tensor([obj['label'] for obj in item['objects']], dtype=torch.float32) for item in batch]\n",
    "    \n",
    "    # Assuming `pad_sequence` is from `torch.nn.utils.rnn`\n",
    "    inputs_padded = pad_sequence(inputs, batch_first=True, padding_value=0.0)\n",
    "    labels_stacked = pad_sequence(labels, batch_first=True, padding_value=-1)  # Assuming -1 for padding labels\n",
    "    \n",
    "    return inputs_padded, labels_stacked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hawk/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-15.8332, -27.5944, -22.8197,  ..., -29.1857, -24.1959, -27.7856]],\n",
      "\n",
      "         [[-33.2669, -22.0209, -21.0198,  ..., -12.4791, -19.2950, -37.5013]],\n",
      "\n",
      "         [[-36.8685, -10.0651, -16.8682,  ..., -23.9756, -25.5641, -26.2080]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-13.6355, -22.4010, -15.2205,  ..., -20.9831,  -8.2578, -20.8184]],\n",
      "\n",
      "         [[-14.4204, -24.5969,  -7.7205,  ..., -26.4825, -24.9407, -28.4674]],\n",
      "\n",
      "         [[-30.8561, -28.0231, -18.4204,  ..., -21.4272, -30.5318, -48.2270]]],\n",
      "\n",
      "\n",
      "        [[[-10.1008, -19.9613, -15.3482,  ..., -19.0032, -30.9790, -12.6801]],\n",
      "\n",
      "         [[ -7.1256, -15.8463, -18.0300,  ..., -14.2009, -11.8860,  -7.1965]],\n",
      "\n",
      "         [[-14.4611, -15.5691, -26.6829,  ..., -15.8769, -10.5550, -26.4153]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ -7.4395, -20.7321,  -9.3639,  ...,  -5.1072,  -3.2042, -21.3909]],\n",
      "\n",
      "         [[ -3.8068, -21.2065, -11.6436,  ...,  -7.7607,  -9.2954, -23.5890]],\n",
      "\n",
      "         [[-21.4882, -35.1383, -25.5433,  ..., -21.3057, -33.3178, -21.6531]]],\n",
      "\n",
      "\n",
      "        [[[-21.2620, -15.6158, -22.9999,  ..., -24.2301, -19.9200,  -9.9087]],\n",
      "\n",
      "         [[-17.4956, -25.4240, -19.0147,  ..., -18.4814, -16.6826, -13.8028]],\n",
      "\n",
      "         [[-18.8019, -28.6718, -18.7856,  ..., -20.9645, -25.6777, -21.2099]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-19.7614, -18.4375,  -9.3716,  ..., -19.9979, -10.6799, -16.1025]],\n",
      "\n",
      "         [[ -9.7363, -28.1822, -10.2209,  ..., -17.2845,  -0.4769, -14.5377]],\n",
      "\n",
      "         [[-22.5913, -44.1384, -25.1926,  ..., -33.7956, -14.5166, -31.0188]]],\n",
      "\n",
      "\n",
      "        [[[-18.5175, -17.8277, -13.5422,  ..., -16.8634, -25.1351, -16.9561]],\n",
      "\n",
      "         [[-20.3489, -15.6663, -17.3288,  ..., -10.0392, -17.5249, -13.9790]],\n",
      "\n",
      "         [[-14.6713, -23.7172, -13.9038,  ..., -13.4378, -22.0246, -26.1688]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ -7.7863, -21.8699, -18.4270,  ..., -13.8407,  -9.9803, -21.8943]],\n",
      "\n",
      "         [[-14.9956, -23.1780, -25.7943,  ..., -12.3088, -17.0470, -27.2717]],\n",
      "\n",
      "         [[-28.1247, -40.3917, -34.5110,  ..., -21.1662, -35.7873, -33.2757]]]])\n",
      "tensor([[ 5.,  2.,  2.,  2., -1.],\n",
      "        [ 2.,  2.,  2.,  2.,  2.],\n",
      "        [ 2.,  2., -1., -1., -1.],\n",
      "        [ 0.,  5.,  2., -1., -1.]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [64, 4, 7, 7], expected input[4, 256, 1, 64] to have 4 channels, but got 256 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[88], line 29\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(labels)\n\u001b[1;32m     27\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 29\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     31\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/models/resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/models/resnet.py:268\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;66;03m# See note [TorchScript super()]\u001b[39;00m\n\u001b[0;32m--> 268\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    269\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(x)\n\u001b[1;32m    270\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 4, 7, 7], expected input[4, 256, 1, 64] to have 4 channels, but got 256 channels instead"
     ]
    }
   ],
   "source": [
    "model = models.resnet18(pretrained=False)\n",
    "num_classes = 6  \n",
    "model.conv1 = nn.Conv2d(4, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "model = model.to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 10  \n",
    "dataset = RaddetDataset(all_loaded_RAD_files, all_loaded_GT_files, stereo_files_paths)\n",
    "dataloader = DataLoader(dataset, batch_size=4, collate_fn=collate)\n",
    "test_dataset = RaddetDataset(all_loaded_RAD_files, all_loaded_GT_files, stereo_files_paths)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=4, collate_fn=collate)  # Adjust batch_size as needed\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  \n",
    "    running_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_labels = 0\n",
    "\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        print(inputs)\n",
    "        print(labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        preds = torch.sigmoid(outputs) > 0.5  \n",
    "        total_correct += (preds == labels).sum().item()\n",
    "        total_labels += labels.numel()\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    epoch_acc = total_correct / total_labels\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"Train-   Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}\")\n",
    "    model.eval()  \n",
    "    running_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_labels = 0\n",
    "    with torch.no_grad():  \n",
    "        for inputs, labels in test_dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            test_preds = torch.sigmoid(outputs) > 0.5\n",
    "            total_correct += (test_preds == labels).sum().item()\n",
    "            total_labels += labels.numel()\n",
    "\n",
    "    epoch_loss = running_loss / len(test_dataloader)\n",
    "    epoch_acc = total_correct / total_labels\n",
    "    print(f'Test-   Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [64, 3, 7, 7], expected input[16, 480, 1280, 3] to have 3 channels, but got 480 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[90], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     24\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 25\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     27\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/models/resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/models/resnet.py:268\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;66;03m# See note [TorchScript super()]\u001b[39;00m\n\u001b[0;32m--> 268\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    269\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(x)\n\u001b[1;32m    270\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 3, 7, 7], expected input[16, 480, 1280, 3] to have 3 channels, but got 480 channels instead"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),  # Resize images to 224x224 for ResNet\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize for pre-trained ResNet\n",
    "])\n",
    "\n",
    "dataset = RaddetDataset(all_loaded_RAD_files, all_loaded_GT_files, stereo_files_paths)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True, num_workers=4, collate_fn=custom_collate_fn)\n",
    "num_classes = len(dataset.classes_list)\n",
    "model = models.resnet18(pretrained=True)\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 25\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch in dataloader:\n",
    "        inputs = batch['images'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(dataloader)}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
